<?xml version="1.0" encoding="UTF-8" ?>
<ChoregrapheProject xmlns="http://www.ald.softbankrobotics.com/schema/choregraphe/project.xsd" xar_version="3">
  <Box name="root" id="-1" localization="8" tooltip="Root box of Choregraphe&apos;s behavior. Highest level possible." x="0" y="0">
    <bitmap>media/images/box/root.png</bitmap>
    <script language="4">
      <content>
        <![CDATA[]]>
      </content>
    </script>
    <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
    <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
    <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
    <Input name="FrontTactilTouched" type="0" type_size="1" nature="4" stm_value_name="FrontTactilTouched" inner="1" tooltip="FrontTactilTouched desc" id="4" />
    <Input name="MiddleTactilTouched" type="0" type_size="1" nature="4" stm_value_name="MiddleTactilTouched" inner="1" tooltip="MiddleTactilTouched desc" id="5" />
    <Input name="RearTactilTouched" type="0" type_size="1" nature="4" stm_value_name="RearTactilTouched" inner="1" tooltip="RearTactilTouched desc" id="6" />
    <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="7" />
    <Timeline enable="0">
      <BehaviorLayer name="behavior_layer1">
        <BehaviorKeyframe name="keyframe1" index="1">
          <Diagram>
            <Box name="Say" id="2" localization="8" tooltip="Say some text. The text can be localized." x="943" y="208">
              <bitmap>media/images/box/interaction/say.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import time

class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)

    def onLoad(self):
        self.tts = self.session().service('ALTextToSpeech')
        self.ttsStop = self.session().service('ALTextToSpeech') #Create another service as wait is blocking if audioout is remote
        self.bIsRunning = False
        self.ids = []

    def onUnload(self):
        for id in self.ids:
            try:
                self.ttsStop.stop(id)
            except:
                pass
        while( self.bIsRunning ):
            time.sleep( 0.2 )

    def onInput_onStart(self):
        self.bIsRunning = True
        try:
            sentence = "\RSPD="+ str( self.getParameter("Speed (%)") ) + "\ "
            sentence += "\VCT="+ str( self.getParameter("Voice shaping (%)") ) + "\ "
            sentence += self.getParameter("Text")
            sentence +=  "\RST\ "
            id = self.tts.pCall("say",str("sentence"))
            self.ids.append(id)
            self.tts.wait(id)
        finally:
            try:
                self.ids.remove(id)
            except:
                pass
            if( self.ids == [] ):
                self.onStopped() # activate output of the box
                self.bIsRunning = False

    def onInput_onStop(self):
        self.onUnload()]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when Diagram is loaded." id="1" />
              <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this Input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this Input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when Box behavior is finished." id="4" />
              <Parameter name="Voice shaping (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="150" tooltip='Used to modify at runtime the voice feature (tone, speed). In a slighty&#x0A;different way than pitch and speed, it gives a kind of &quot;gender or age&#x0A;modification&quot; effect.&#x0A;&#x0A;For instance, a quite good male derivation of female voice can be&#x0A;obtained setting this parameter to 78%.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the&#x0A;speed parameter. For example, if you want to decrease by 20% the voice&#x0A;shaping, you will have to increase by 20% the speed to keep a constant&#x0A;average speed.' id="5" />
              <Parameter name="Speed (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="200" tooltip="Changes the speed of the voice.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the voice&#x0A;shaping parameter. For example, if you want to increase by 20% the speed, you&#x0A;will have to decrease by 20% the voice shaping to keep a constant average&#x0A;speed." id="6" />
              <Parameter name="Text" inherits_from_parent="0" content_type="5" value="Hello" default_value="" tooltip="The text you want to say. Don&apos;t forget to translate it!" id="7" />
              <Resource name="Speech" type="Lock" timeout="0" />
            </Box>
            <Box name="AskGpt" id="1" localization="8" tooltip="This box contains a basic python script and can be used to create any python script box you would like.&#x0A;&#x0A;To edit its script, double-click on it." x="230" y="215">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[import json
import time
import qi.logging
from threading import Thread

try:
    from urllib.request import Request, urlopen
except ImportError:
    from urllib2 import Request, urlopen

AUDIO_FILE = "test.wav"
api_key = "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"
char_prompt = "Du bist ein humorvoller Chatbot in der Form eines Nao-Roboters für die Stadtteilbibliothek Buch in Berlin. Du informierst die Nutzer über Öffnungszeiten, Angebote und Veranstaltungen der Bibliothek und interagierst dabei auf witzige und charmante Weise. Öffnungszeiten: Montag: 13:00–19:00 Uhr; Dienstag: 10:00–16:00 Uhr; Mittwoch: 13:00–17:00 Uhr; Donnerstag: 13:00–19:00 Uhr; Freitag: 13:00–19:00 Uhr; Samstag: 10:00–15:00 Uhr. Angebote: Vielfältiges Medienangebot, darunter Bücher, Hörbücher, DVDs, fremdsprachige Bestände (Englisch, Polnisch, Arabisch, Persisch, Ukrainisch, Russisch, Türkisch, Spanisch), Zeitschriften, Spiele, Konsolenspiele (PlayStation 4, Nintendo Switch/DS/3DS), Tiptoi-Stifte und Bücher, Tolino eBook-Reader, Tonieboxen. Spezielle Angebote für Kinder und Jugendliche: Vermittlung von Medienkompetenz und Diversität durch Coding- und Robotikangebote (z.B. Dash und Dot, Bee Bots, Lego Education Sets), Gaming-Nachmittage mit Nintendo Switch, PlayStation 5, Gaming-Laptops und VR-Brillen. Veranstaltungen: Regelmäßige Events wie Bastelstation (jeden 1. und 3. Freitag des Monats, 15:30–16:30 Uhr), Bilderbuchkino, Senioren-Singen, Digital-Zebra (Digital-Beratung), Digital-Café, Frauen*-Workshops, Tabletkurse für Senioren, Human Library (mehrsprachig), Graffiti-Workshops für Jugendliche, Puppenbau-Workshops, Weihnachtsbasteln, Kindertheater, Spiele-Café (jeden 2. und 4. Mittwoch im Monat, 13:00–17:00 Uhr). Fahrradbibliothek: Von März bis Oktober unterwegs mit Riesenspielen und Medien zum Ausleihen, besucht verschiedene Orte in Buch, darunter Unterkünfte für Geflüchtete und die HOWOGE-Siedlung. Du nutzt diese Informationen, um den Nutzern auf humorvolle Weise weiterzuhelfen und sie über die Angebote der Bibliothek zu informieren."
convo = [{"role": "system", "content": char_prompt}]

class GestureAnimation:
    def __init__(self, session):
        self.motion = session.service("ALMotion")
        self.posture = session.service("ALRobotPosture")
        self.behavior_manager = session.service("ALBehaviorManager")
        self.running = False

    def start_animation(self):
        self.running = True
        self.motion.setStiffnesses("Body", 1.0)  # Ensure joints are stiffened

        # Gesture loop
        while self.running:
            # Right arm moves up and down
            self.motion.angleInterpolation(
                ["RShoulderPitch", "RShoulderRoll"],
                [[0.2, 1.0], [0.1, -0.3]],
                [[1.0, 1.5], [1.0, 1.5]],
                True
            )
            # Head tilts slightly
            self.motion.angleInterpolation(
                "HeadYaw",
                [0.2, -0.2],
                [1.5, 2.0],
                True
            )
            # Left arm moves up and down
            self.motion.angleInterpolation(
                ["LShoulderPitch", "LShoulderRoll"],
                [[0.2, 1.0], [0.1, -0.3]],
                [[1.0, 1.5], [1.0, 1.5]],
                True
            )

    def stop_animation(self):
        self.running = False
        self.motion.setStiffnesses("Body", 0.0)  # Release stiffness


def call_chatgpt_api(prompt, api_key):
    #"messages": [{"role": "system", "content": char_prompt}, {"role": "user", "content": prompt}],
    convo.append({"role": "user", "content": prompt})
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": "Bearer " + api_key,
        "Content-Type": "application/json"
    }
    data = json.dumps({
        "model": "gpt-3.5-turbo",
        "messages": convo,
        "max_tokens": 100,
        "temperature": 0.7
    }).encode('utf-8')

    try:
        request = Request(url, data=data, headers=headers)
        response = urlopen(request)
        result = json.load(response)
        return result['choices'][0]['message']['content']
    except Exception as e:
        return "Error calling ChatGPT API: " + str(e)

def transcribe_with_openai(file_path, api_key):
    url = "https://api.openai.com/v1/audio/transcriptions"
    boundary = "----WebKitFormBoundary7MA4YWxkTrZu0gW"
    headers = {
        "Authorization": "Bearer " + api_key,
        "Content-Type": "multipart/form-data; boundary=" + boundary
    }

    try:
        # Prepare the audio file as multipart/form-data
        with open(file_path, "rb") as audio_file:
            data = b''
            # Add file field
            data += "--" + boundary + "\r\n"
            data += "Content-Disposition: form-data; name=\"file\"; filename=\"" + AUDIO_FILE + "\"\r\n"
            data += "Content-Type: audio/wav\r\n\r\n"
            data += audio_file.read() + b"\r\n"

            # Add model field
            data += "--" + boundary + "\r\n"
            data += "Content-Disposition: form-data; name=\"model\"\r\n\r\n"
            data += "whisper-1\r\n"

            # End boundary
            data += "--" + boundary + "--\r\n"

        # Send the HTTP request
        request = Request(url, data=data, headers=headers)
        response = urlopen(request)
        result = json.load(response)

        # Return the transcription text
        return result.get("text", "No transcription found")
    except Exception as e:
        return "Error transcribing audio: " + str(e)  # Convert error to string


class MyClass(GeneratedClass):
    recIsRunning = False

    def __init__(self):
        GeneratedClass.__init__(self)

    def onLoad(self):
        # Initialization code here
        self.tts = self.session().service('ALTextToSpeech')
        self.audiorec = self.session().service("ALAudioRecorder")
        self.audioplayer = self.session().service("ALAudioPlayer")
        self.logmanager = self.session().service("LogManager")
        #self.motionClass = GestureAnimation(self.session())
        #self.motionClass.start_animation()
        pass

    def onUnload(self):
        # Clean-up code here
        pass

    def onInput_onStart(self):
        api_key = "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"
        prompt = "Was ist die Haubtstadt von Deutschland?"
        channels = [0, 0, 1, 0]

        self.audiorec.startMicrophonesRecording("/home/nao/test.wav", "wav", 16000, channels);
        time.sleep(5)
        self.audiorec.stopMicrophonesRecording();

        text = transcribe_with_openai("/home/nao/test.wav", api_key)
        #self.say(text)

        #self.audioplayer.playFile("/home/nao/test.wav")
        self.delete_audio("/home/nao/test.wav")

        response = call_chatgpt_api(text, api_key)
        self.say(response)
        pass

    def onInput_onStop(self):
        self.onUnload()  # Reuse clean-up as the box is stopped
        self.onStopped()  # Activate the output of the box

    def on_text_input(self, value):
        print("User said:", value)
        response = call_chatgpt_api(value, "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")
        print("ChatGPT Response:", response)
        self.speak(response)

    def onInput_middel(self):
        self.say("middel")
        self.audiorec.stopMicrophonesRecording();
        if self.recIsRunning == True:
            self.recIsRunning = False
            #self.logmanager.info("transcribing the audio file using openai")
            text = transcribe_with_openai("/home/nao/test.wav", api_key)
            #self.say(text)
            #self.logmanager.info(text)
            response = call_chatgpt_api(text, api_key)
            self.say(response)
            self.delete_audio("/home/nao/test.wav")

    def onInput_front(self):
        if self.recIsRunning == False:
            self.recIsRunning = True
            self.say("Stell mir eine Frage.")
            #self.logmanager.info("Stell mir eine Frage")
            time.sleep(2)
            channels = [0, 0, 1, 0]
            self.audiorec.startMicrophonesRecording("/home/nao/test.wav", "wav", 16000, channels)

    def onInput_back(self):
        global convo
        self.say("auha... Huch wo bin ich?")
        convo = [{"role": "system", "content": char_prompt}]
        self.audiorec.stopMicrophonesRecording();
        self.delete_audio("/home/nao/test.wav")

    def say(self, value):
        self.tts.pCall("say", str(value))

    def delete_audio(self, file_path):
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                print("Deleted audio file: {file_path}")
            else:
                print("Audio file file_path does not exist.")
        except Exception as e:
            print("Error deleting audio file: {e}")]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Input name="front" type="1" type_size="1" nature="1" inner="0" tooltip="" id="4" />
              <Input name="middel" type="1" type_size="1" nature="1" inner="0" tooltip="" id="5" />
              <Input name="back" type="1" type_size="1" nature="1" inner="0" tooltip="" id="6" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="7" />
            </Box>
            <Box name="GetVoice" id="3" localization="8" tooltip="This box contains a basic python script and can be used to create any python script box you would like.&#x0A;&#x0A;To edit its script, double-click on it." x="557" y="61">
              <bitmap>media/images/box/box-python-script.png</bitmap>
              <script language="4">
                <content>
                  <![CDATA[def get_user_input():
    try:
        # Connect to ALMemory proxy
        memory = ALProxy("ALMemory", "127.0.0.1", 9559)

        # Connect to ALTextToSpeech proxy for feedback
        self.tts = self.session().service('ALTextToSpeech')

        self.tts.say("Please type your input in the dialog box.")

        # Monitor for input from the dialog box
        while True:
            user_input = memory.getData("Dialog/LastInput")
            if user_input:  # Check if input is available
                tts.say("You said: " + user_input)
                return user_input

            time.sleep(0.5)  # Avoid busy-waiting

    except Exception as e:
        print("Error: " + e)
        return None]]>
                </content>
              </script>
              <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
              <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
              <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
              <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
            </Box>
            <Link inputowner="0" indexofinput="7" outputowner="1" indexofoutput="7" />
            <Link inputowner="1" indexofinput="5" outputowner="0" indexofoutput="5" />
            <Link inputowner="1" indexofinput="4" outputowner="0" indexofoutput="4" />
            <Link inputowner="1" indexofinput="6" outputowner="0" indexofoutput="6" />
          </Diagram>
        </BehaviorKeyframe>
      </BehaviorLayer>
    </Timeline>
  </Box>
</ChoregrapheProject>
